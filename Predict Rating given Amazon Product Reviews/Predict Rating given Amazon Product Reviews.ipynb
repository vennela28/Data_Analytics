{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Fine Food Reviews Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute Information:\n",
    "    1. Id\n",
    "    2. ProductID - unique identifier for the product\n",
    "    3. UserID - unique identifier for the user\n",
    "    4. ProfileName\n",
    "    5. HelpfulnessNumerator - number of users who found the review useful\n",
    "    6. HelpfulnessDenominator - number of users indicating whether they found the review helpful or not\n",
    "    7. Score - rating between 1 & 5\n",
    "    8. Time - timestamp of the review\n",
    "    9. Summary - brief summary of the review\n",
    "    10. Text - text of the review.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a review, determine whether the review is positive (Rating of 4 / 5) or negative (Rating of 1 / 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Using the SQLite table to read data\n",
    "con = sqlite3.connect('./database.sqlite')\n",
    "\n",
    "# Filtering only positive and negative reviews\n",
    "filtered_data = pd.read_sql_query(\"\"\"SELECT * FROM Reviews WHERE Score != 3\"\"\", con)\n",
    "\n",
    "# Give reviews with Score > 3 a positive rating, and reviews with Score < 3 a negative rating.\n",
    "def partition(x):\n",
    "    if x < 3:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'positive'\n",
    "    \n",
    "# Changing reviews with score less than 3 to be positive and vice-versa\n",
    "actualScore = filtered_data['Score']\n",
    "positiveNegative = actualScore.map(partition)\n",
    "filtered_data['Score'] = positiveNegative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator     Score        Time  \\\n",
       "0                     1                       1  positive  1303862400   \n",
       "1                     0                       0  negative  1346976000   \n",
       "2                     1                       1  positive  1219017600   \n",
       "3                     3                       3  negative  1307923200   \n",
       "4                     0                       0  positive  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.shape\n",
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Cleaning: Deduplication</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the real world, when we do machine learning we spend 20% to 30% time in Data Cleaning & Preprocessing. \n",
    "\n",
    "The Dataset has many duplicate rows.\n",
    "\n",
    "Hence it is necessary to remove duplicates in order to get unbiased results for the analysis of the data. It is not adding any value to the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78445</td>\n",
       "      <td>B000HDL1RQ</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138317</td>\n",
       "      <td>B000HDOPYC</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138277</td>\n",
       "      <td>B000HDOPYM</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73791</td>\n",
       "      <td>B000HDOPZG</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155049</td>\n",
       "      <td>B000PAQ75C</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId         UserId      ProfileName  HelpfulnessNumerator  \\\n",
       "0   78445  B000HDL1RQ  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "1  138317  B000HDOPYC  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "2  138277  B000HDOPYM  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "3   73791  B000HDOPZG  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "4  155049  B000PAQ75C  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "\n",
       "   HelpfulnessDenominator  Score        Time  \\\n",
       "0                       2      5  1199577600   \n",
       "1                       2      5  1199577600   \n",
       "2                       2      5  1199577600   \n",
       "3                       2      5  1199577600   \n",
       "4                       2      5  1199577600   \n",
       "\n",
       "                             Summary  \\\n",
       "0  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "1  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "2  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "3  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "4  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "\n",
       "                                                Text  \n",
       "0  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "1  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "2  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "3  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "4  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display = pd.read_sql_query(\"\"\"\n",
    "SELECT * FROM Reviews WHERE Score != 3 AND \n",
    "UserId = 'AR5J8UI46CURR' ORDER BY ProductID\"\"\", con)\n",
    "display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reduce the redundancy it was decided to eliminate the rows having same parameters.\n",
    "\n",
    "Method is as follows:\n",
    "    1. Sort the data according to ProductId and then just keep the first similar product review and delete the others.\n",
    "    \n",
    "This method ensures that there is only one representative for each product and deduplication without sorting would lead to possibility of different representatives still existing for the same product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data according to ProductId in ascending order.\n",
    "sorted_data = filtered_data.sort_values('ProductId', axis = 0, ascending = True, inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364173, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deduplication of entries - # of rows left in the dataset\n",
    "final = sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep = 'first', inplace = False)\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.25890143662969"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking to see how much amount of data still remains after cleaning the duplicates \n",
    "# Retained 69% of the data\n",
    "(final['Id'].size * 1.0) / (filtered_data['Id'].size * 1.0) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Observation 2</h4> HelpfulnessNumerator should always be lesser than HelpfulnessDenominator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64422</td>\n",
       "      <td>B000MIDROQ</td>\n",
       "      <td>A161DK06JJMCYF</td>\n",
       "      <td>J. E. Stephens \"Jeanne\"</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1224892800</td>\n",
       "      <td>Bought This for My Son at College</td>\n",
       "      <td>My son loves spaghetti so I didn't hesitate or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44737</td>\n",
       "      <td>B001EQ55RW</td>\n",
       "      <td>A2V0I904FH7ABY</td>\n",
       "      <td>Ram</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1212883200</td>\n",
       "      <td>Pure cocoa taste with crunchy almonds inside</td>\n",
       "      <td>It was almost a 'love at first bite' - the per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id   ProductId          UserId              ProfileName  \\\n",
       "0  64422  B000MIDROQ  A161DK06JJMCYF  J. E. Stephens \"Jeanne\"   \n",
       "1  44737  B001EQ55RW  A2V0I904FH7ABY                      Ram   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     3                       1      5  1224892800   \n",
       "1                     3                       2      4  1212883200   \n",
       "\n",
       "                                        Summary  \\\n",
       "0             Bought This for My Son at College   \n",
       "1  Pure cocoa taste with crunchy almonds inside   \n",
       "\n",
       "                                                Text  \n",
       "0  My son loves spaghetti so I didn't hesitate or...  \n",
       "1  It was almost a 'love at first bite' - the per...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display = pd.read_sql_query(\"\"\"\n",
    "SELECT * FROM Reviews \n",
    "WHERE Score != 3 AND Id = 44737 OR Id = 64422 \n",
    "ORDER BY ProductID\"\"\", con)\n",
    "display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep those rows where HelpfulnessDenomintor >= HelpfulnessNumerator\n",
    "final = final[final.HelpfulnessNumerator <= final.HelpfulnessDenominator]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Next Phase of Preprocessing</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364171, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "positive    307061\n",
       "negative     57110\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(final.shape)\n",
    "\n",
    "# How many positives and negatives are present in our dataset after removing the duplicates \n",
    "# and other discrebancies\n",
    "final['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the 8 features as input we have to predict the sentiment polarity (+ve/-ve).\n",
    "We are determining the polarity by the Score.\n",
    "\n",
    "The most useful features are Summary and Text.\n",
    "\n",
    "Given any problem if we can convert into the problem of vectors, we can leverage the power of Linear Algebra.\n",
    "\n",
    "How do you convert text into numerical vectors?\n",
    "\n",
    "Convert Review Text into d-D vector in d-D space.\n",
    "\n",
    "Suppose we have many vectors. Each point represents a d-D representation of a review in d-D space.\n",
    "\n",
    "Draw a hyperplane 'pi' separating all positive reviews and all negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Converting Review-text into a d-D vector\n",
    "2. Finding a plane to separate the reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Rules/Properties of this conversion </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have 3 reviews - $r_{1}$, $r_{2}$ and $r_{3}$. d-D representation of vectors for $r_{1}$ -> $v_{1}$, $r_{2}$ -> $v_{2}$, $r_{3}$ -> $v_{3}$.\n",
    "\n",
    "If $r_{1}$ and $r_{2}$ are more similar semantically that $r_{1}$ and $r_{3}$, i.e. Eng. sim($r_{1}$, $r_{2}$) > Eng. sim($r_{1}$, $r_{3}$) then the dist($v_{1}$, $v_{2}$) < dist($v_{1}$, $v_{3}$). \n",
    "\n",
    "If $r_{1}$ & $r_{2}$ are more similar, $v_{1}$ and $v_{2}$ must be close i.e. <b>length($v_{1}$  - $v_{2}$) < length($v_{1}$ - $v_{3}$)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>find {text -> d-D vector} such that similiar text must be closer geometrically.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplest Technique to convert text to a numerical vector is <b>Bag Of Words(BoW)</b>\n",
    "\n",
    "$r_{1}$: This pasta is very tasty and affordable.\n",
    "\n",
    "$r_{2}$: This pasta is not tasty and is affordable.\n",
    "\n",
    "$r_{3}$: This pasta is delicious and cheap.\n",
    "\n",
    "$r_{4}$: Pasta is tasty and pasta tastes good.\n",
    "\n",
    "In NLP, a review is known as a document. Set of documents is called<b> corpus</b>.\n",
    "\n",
    "1. Constructing a dictionary - set of all unique words in the reviews. \n",
    "{This, pasta, ...}\n",
    "2. Construct Vector $v_{i}$ of size 'd'. Each word is a different dimension and each cell corresponds to # of times the word occurs in the review/document $r_{i}$.\n",
    "\n",
    "$v_{i}$ is a sparse vector - most of the elements are zero.\n",
    "\n",
    "<b>Objective of BoW:</b> Similar text must result as closer vectors.\n",
    "\n",
    "BoW is thought of counting the common words when all the values exist only once. How many common words exist? \n",
    "\n",
    "BoW does not work very well when there are small changes in the terminology we are using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Binary BoW</b> or <b>Boolean BoW</b> is a variation of BoW. Instead of putting count, we put 1 if the word occurs atleast once and 0 if the word doesn't exist. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||$v_{1}$ - $v_{2}$|| = $\\sqrt number of different words$ between documents/reviews $r_{1}$ and $r_{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the words like {This, is, and} do not matter much. What matters the most is the non-trivial words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the trivial words <b>Stop-words</b>.\n",
    "\n",
    "If I remove the Stop-words, BoW vector will be smaller and more meaningful. You throw these Stop-words while constructing the vector.\n",
    "\n",
    "In English 'not' is also considered as a Stop-word.\n",
    "\n",
    "<b>So, removing the stop-words is not always the best choice.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Text Pre-processing steps</h4>\n",
    "1. Removing <b>Stop-words</b>.\n",
    "2. Convert all your words <b>lowercase</b>.\n",
    "3. <b>Stemming</b>: words coming from the same base word in English. Eg. tastes, tasful, tasty -> tast. Convert all these words into their common form i.e. taste and replace them with the common form. Related words are considered as single root word.\n",
    "Stemming algorithms - PorterStemmer, SnowballStemmer \n",
    "4. <b>Lemmatization</b>: breaking up a sentence into words. A space is used to break the sentence into words. \n",
    "Eg. This pasta is very tasty. This is the best in New York.\n",
    "But there can be complex words like New York. It is a location. \n",
    "Often times we break the sentence but there are lemmatizers available which will group New York into 1 word. \n",
    "5. Tasty and delicious are synonyms - very similar in meaning. But in BoW, we are considering them as 2 different words which are nowhere related because they are 2 different dimensions. In BoW we are not taking semantic meaning of words into consideration. A technique called <b>Word2Vec</b> where we try to get semantic meaning of these words into consideration when we build vectors of text.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>BoW + Text Preprocessing </b>\n",
    "Converting text to a d-D vector which doesn't guarantee semantic meaning of words will be at the same place. \n",
    "$r_{1}$ and $r_{3}$ are sematically same because our algorithm still think they are different. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The <u>drawback</u> of BoW is <i>it doesn't take semantic meaning into consideration</i>.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uni-gram, Bi-gram, n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$r_{1}$: This pasta is very tasty and affordable.\n",
    "\n",
    "$r_{2}$: This pasta is not tasty and is affordable.\n",
    "\n",
    "After removing stop-words $v_{1}$ and $v_{2}$ are exactly the same => $r_{1}$ and $r_{2}$ are very similar which is not TRUE. \n",
    "\n",
    "$r_{1}$ and $r_{2}$ are completely opposite. \n",
    "\n",
    "<b>Uni-gram</b>: Each word is considered as a dimension.\n",
    "<b>Bi-gram</b>: Pairs of consecutive words is considered as a dimension.\n",
    "<b>Tri-gram</b>: 3 consecutive words is considered as a dimension.\n",
    "<b>n-gram</b>: n consecutive words is considered as a dimension.\n",
    "\n",
    "<b>Why n-gram?</b> Uni-gram based BoW discards the sequence information. But using bi-gram, tri-gram or n-gram we are trying to retain some of the partial sequence information. \n",
    "\n",
    "Bi-gram, Tri-gram or n-gram can be easily encorporated into BoW.  \n",
    "\n",
    "<b># of bi-grams >= # of uni-grams</b> because the number of pairs of consecutive words is greater than or equal to uni-grams. \n",
    "\n",
    "<b># of n-grams >= ... >= # of tri-grams >= # of bi-grams >= # of uni-grams</b>\n",
    "\n",
    "For n-grams, where n > 1, dimensionality 'd' increases drastically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF(Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variation of BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume we have 'N' documents / reviews. Each review is a combination of words.\n",
    "\n",
    "Let us assume $r_{1}$ has some words. Similarly, other documents too.\n",
    "\n",
    "$r_{1}$: $W_{1}$, $W_{2}$, $W_{3}$, $W_{2}$, $W_{5}$             --> 5 words\n",
    "\n",
    "$r_{2}$: $W_{1}$, $W_{3}$, $W_{4}$, $W_{5}$, $W_{6}$, $W_{2}$    --> 6 words\n",
    "\n",
    "$r_{3}$: \n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "$r_{N}$:\n",
    "\n",
    "TF($W_{i}$, $r_{j}$) = # of times $W_{i}$ occurs in $r_{j}$ / total number of words in $r{j}$\n",
    "TF($W_{2}$, $r_{1}$) = 2 / 5\n",
    "\n",
    "<b>0 <= TF($W_{i}$, $r_{j}$) <= 1 </b> Can be interpreted as Probability.\n",
    "\n",
    "<u>BoW</u> and <u>TF-IDF</u> are techniques done on the text for <i><b>Information Retrieval</b></i> (sub-area of NLP).\n",
    "\n",
    "TF can be thought of as how often does $W_{i}$ occur in $r_{j}$. If it has all the same words then it has a TF of 1 else if the word occurs a very few times, the TF has a very small value. <b> More often the word occurs, the higher the frequency. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Term Frequency can be thought of as the probability of finding a word $W_{i}$ in a document $r_{j}$. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>IDF- Inverse Document Frequency</i></b> is for a word $W_{i}$ in a corpus.\n",
    "\n",
    "Suppose Dataset/Corpus ($D_{c}$) has the following documents:\n",
    " \n",
    "$r_{1}$: $W_{1}$, $W_{2}$, $W_{3}$, $W_{2}$, $W_{5}$             --> 5 words\n",
    "\n",
    "$r_{2}$: $W_{1}$, $W_{3}$, $W_{4}$, $W_{5}$, $W_{6}$, $W_{2}$    --> 6 words\n",
    "\n",
    "$r_{3}$: \n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "$r_{N}$:\n",
    "\n",
    "<b>IDF($W_{i}$, $D_{c}$) = log(N/$n_{i}$)</b>, where N is the number of documents and $n_{i}$ is the number of documents which contain the word $W_{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Since $n_{i}$ <= N, N/$n_{i}$ >= 1. So, log(N/$n_{i}$) >= 0</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. IDF >= 0\n",
    "2. If $n_{i}$ increases, then N/$n_{i}$ decreases. Here monotonic function log(N/$n_{i}$) decreases. \n",
    "<b>If $W_{i}$ is more frequent in $D_{c}$, the IDF is lower.</b> Hence, if IDF increases, $n_{i}$ decreases and vice-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>If $W_{i}$ is more frequent, IDF will be low and if $W_{i}$ is very rare, IDF will be high.</i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given documents {$r_{1}$, $r_{2}$, $r_{3}$,..., $r_{j}$} in $D_{c}$, <b>TF-IDF: TF($W_{i}$, $r_{j}$) * IDF($W_{i}$, $D_{c}$)</b>, TF($W_{i}$, $r_{j}$) is higher if $W_{i}$ is frequent in $r_{j}$ and IDF($W_{i}$, $D_{c}$) is higher when $W_{i}$ is rare in $D_{c}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>TF-IDF gives\n",
    "- gives more importance to rarer words in $D_{c}$.\n",
    "- gives more importance if a word is more frequent in a document/review.</b>\n",
    "\n",
    "But TF-IDF has a <u>drawback</u> that it <i><b>does not</b> take semantic meaning of words</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Word2Vec</b> takes semantic meaning of words into consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm takes a word and converts it into a d-D vector where d is typically, 50, 100, 200 or 300. But this is not a sparse vector. But BoW / TF-IDF represented sentences into sparse vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a 300-D vector. The higher the dimensions, more powerful is the representation.\n",
    "\n",
    "1. If $W_{1}$ and $W_{2}$ are semantically similar, then $v_{1}$ and $v_{2}$ are closer.\n",
    "2. In Word2Vec, it satisfies the relationships. \n",
    "\n",
    "<b>($V_{man}$ - $V_{woman}$) || ($V_{king}$ - $V_{queen}$) </b>\n",
    "\n",
    "Word2Vec learns relationships automatically from raw-text.\n",
    "\n",
    "Word2Vec takes a very large text Corpus as input and for every word it builds a vector. \n",
    "\n",
    "Larger dimensions --> more information rich the vector is. If we have a higher dimensional vector it can learn far more complex relationships. \n",
    "\n",
    "If $D_{c}$ is large, the higher is the dimensionality. \n",
    "\n",
    "Word2Vec looks at sequence information of words. Intuitively, for any word Word2Vec looks at neighborhood of that word. \n",
    "\n",
    "N($W_{i}$) is very similar to N($W_{j}$), then $v_{i}$ is very similar to $v_{j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avg-Word2Vec, tf-idf weighted Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Avg-Word2Vec </h4>\n",
    "\n",
    "Word2Vec takes a word and converts it into a d-D vector. \n",
    "\n",
    "But $r_{i}$ is a sequence of words/sentences.\n",
    "\n",
    "How do I convert my sentences to a vector using Word2Vec?\n",
    "\n",
    "Suppose we have a review $r_{1}$ containing words\n",
    "$r_{1}$: $W_{1}$, $W_{2}$, $W_{1}$, $W_{3}$, $W_{4}$, $W_{5}$\n",
    "\n",
    "Suppose I want to convert $r_{1}$ to $v_{1}$, take the Avg Word2Vec representation. \n",
    "Take the first word $W_{1}$, convert it into a vector as $v_{1}$\n",
    "\n",
    "For each word in $r_{1}$, I am getting a vector representation.\n",
    "\n",
    "W2V($W_{1}$) + W2V($W_{2}$) + W2V($W_{1}$) + W2V($W_{3}$) + W2V($W_{4}$) + W2V($W_{5}$)\n",
    "\n",
    "Each of these vectors will be d-D. Add all these vectors and then divide the sum by the number of words. \n",
    "\n",
    "Suppose in $r_{1}$, there are $n_{1}$ words then $v_{1}$ becomes <b>1/$n_{1}$[W2V($W_{1}$) + W2V($W_{2}$) + W2V($W_{1}$) + W2V($W_{3}$) + W2V($W_{4}$) + W2V($W_{5}$)]</b>\n",
    "\n",
    "$v_{1}$ is the vector representation of review $r_{1}$. This is known as Avg-Word2Vec. It is not perfect but it works well. This is the simplest way to leverage Word2Vec to build sentence vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> tf-idf weighted Word2Vec </h4>\n",
    "\n",
    "Suppose we have a review $r_{1}$ containing words\n",
    "$r_{1}$: $W_{1}$, $W_{2}$, $W_{1}$, $W_{3}$, $W_{4}$, $W_{5}$\n",
    "\n",
    "We compute tf-idf for $r_{1}$ as $t_{1}$, $t_{2}$, $t_{3}$, $t_{4}$, $W_{5}$.\n",
    "\n",
    "When we compute tf-idf-weighted Word2Vec of $r_{1}$\n",
    "\n",
    "<b>tfidf-W2V($r_{1}$) = [$t_{1}$ * W2V($W_{1}$) + $t_{2}$ * W2V($W_{2}$) + $t_{3}$ * W2V($W_{3}$) + $t_{4}$ * W2V($W_{4}$) + $t_{5}$ * W2V($W_{5}$)] / ($t_{1}$ + $t_{2}$ + $t_{3}$ + $t_{4}$ + $t_{5}$)</b> where $t_{i}$ is tf-idf of the word $w_{i}$ in review $r_{1}$ or <b> $t_{i}$ = tf-idf($w_{i}$, $r_{1}$)\n",
    "    \n",
    "    \n",
    "<b>If all $t_{i}$'s are 1, then tfidf-W2V is same as Avg-Word2Vec.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Avg-Word2Vec and tf-idf weighted Word2Vec are simple weighting strategies to convert sentences/paragraphs to vectors.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "final_counts = count_vect.fit_transform(final['Text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(final_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 115281)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_counts.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Text Preprocessing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Begin by removing all the HTML tags.\n",
    "2. Remove any punctuation or a set of special characters like , or . or # etc.\n",
    "3. Check if a word is in simple English and is not alpha-numeric.\n",
    "4. Check to see if the length of the word is greater than 2.\n",
    "5. Convert the word to lowercase. \n",
    "6. Remove Stop-words. \n",
    "7. <b>Snowball Stemming is observed to be better than Porter Stemming</b>\n",
    "\n",
    "After executing the above steps collect the words that are used to describe whether a review is positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "I set aside at least an hour each day to read to my son (3 y/o). At this point, I consider myself a connoisseur of children's books and this is one of the best. Santa Clause put this under the tree. Since then, we've read it perpetually and he loves it.<br /><br />First, this book taught him the months of the year.<br /><br />Second, it's a pleasure to read. Well suited to 1.5 y/o old to 4+.<br /><br />Very few children's books are worth owning. Most should be borrowed from the library. This book, however, deserves a permanent spot on your shelf. Sendak's best.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "i = 0\n",
    "\n",
    "for sentence in final['Text'].values:\n",
    "    if(len(re.findall('<.*?>', sentence))):\n",
    "        print(i)\n",
    "        print(sentence)\n",
    "        break;\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\venne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "{'ain', 'been', 'shouldn', \"mightn't\", 'in', 'such', 'isn', 'did', \"shan't\", 'didn', 'does', 'itself', 'being', 'weren', 'they', 'is', 'be', 'y', \"it's\", 'mightn', \"aren't\", 'his', 'are', 'as', 'once', 'where', 'themselves', 'then', 'll', 'd', 'few', 'ours', \"shouldn't\", 'just', 'himself', 'her', 'most', \"wasn't\", 'their', 'don', 'too', 'both', \"mustn't\", 'am', 'any', 'there', \"couldn't\", 'which', 'during', 'now', 'when', 'wasn', 'some', 'do', 'it', 'if', 'we', \"isn't\", 've', 'over', 'were', \"should've\", 'that', 'again', 'whom', 'shan', 'myself', 'and', 'hers', 'yourself', 'herself', 'doing', 'couldn', 'because', 'hasn', 'can', 'she', 'them', 'up', 'our', 'here', 'o', 'this', 'of', 'your', \"you'd\", 'what', 'other', 'further', 'mustn', 'all', 'i', 's', 'ma', 'me', \"doesn't\", 'an', 'than', 'you', 'has', 'for', 'between', 't', 'him', 'won', 'against', 'no', 'each', \"hasn't\", \"haven't\", 'so', 'before', 'was', 'or', 'by', 'needn', 'above', \"needn't\", 'out', 're', 'doesn', \"hadn't\", 'how', \"you've\", 'm', 'nor', \"that'll\", 'yourselves', 'on', 'not', 'into', \"don't\", 'from', 'with', 'until', 'yours', \"you'll\", 'more', 'wouldn', \"you're\", 'ourselves', \"wouldn't\", 'to', \"weren't\", 'those', \"didn't\", 'only', 'aren', 'own', 'having', 'he', 'these', 'very', 'hadn', 'had', 'under', 'through', 'why', \"won't\", 'theirs', 'off', 'my', 'but', 'have', 'who', 'same', 'will', 'its', 'while', 'below', 'about', 'a', 'down', 'the', 'after', 'should', 'at', \"she's\", 'haven'}\n",
      "----------------------------------------\n",
      "tasti\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop = set(stopwords.words('english')) # set of stopwords\n",
    "sno = nltk.stem.SnowballStemmer('english') # initializing the Snowball Stemmer\n",
    "\n",
    "def cleanhtml(sentence): # function to clean the word of any html tags\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', sentence)\n",
    "    return cleantext\n",
    "\n",
    "def cleanpunc(sentence): # function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]', r'', sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]', r' ', cleaned)\n",
    "    return cleaned\n",
    "\n",
    "print(stop)\n",
    "\n",
    "print(\"----------------------------------------\")\n",
    "\n",
    "print(sno.stem('tasty'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-grams & n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Motivation</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the list of words describing positive and negative reviews let us analyze them.\n",
    "\n",
    "We begin analysis by getting the frequency distribution of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "str1 = ' '\n",
    "final_string = []\n",
    "all_positive_words = []\n",
    "all_negative_words = []\n",
    "s = ''\n",
    "for sentence in final['Text'].values:\n",
    "    filtered_sentence = []\n",
    "    #print(sentence)\n",
    "    sentence = cleanhtml(sentence)\n",
    "    for w in sentence.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if((cleaned_words.isalpha()) & (len(cleaned_words) > 2)):\n",
    "                if(cleaned_words.lower() not in stop):\n",
    "                    s = (sno.stem(cleaned_words.lower())).encode('utf8')\n",
    "                    filtered_sentence.append(s)\n",
    "                    if(final['Score'].values)[i] == 'positive':\n",
    "                        all_positive_words.append(s)\n",
    "                    if(final['Score'].values)[i] == 'negative':\n",
    "                        all_negative_words.append(s)\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "    str1 = b\" \".join(filtered_sentence)\n",
    "    \n",
    "    final_string.append(str1)\n",
    "    i += 1                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a column of CleanedText\n",
    "final['CleanedText'] = final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.head(3)\n",
    "\n",
    "conn = sqlite3.connect('final.sqlite')\n",
    "c = conn.cursor()\n",
    "conn.text_factory = str\n",
    "final.to_sql('Reviews', conn, flavor = None, schema = None, if_exists = 'replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common Positive words used:  [(b'like', 139429), (b'tast', 129047), (b'good', 112766), (b'flavor', 109624), (b'love', 107357), (b'use', 103888), (b'great', 103870), (b'one', 96726), (b'product', 91033), (b'tri', 86791), (b'tea', 83888), (b'coffe', 78814), (b'make', 75107), (b'get', 72125), (b'food', 64802), (b'would', 55568), (b'time', 55264), (b'buy', 54198), (b'realli', 52715), (b'eat', 52004)]\n",
      "Most common Negative words used:  [(b'tast', 34585), (b'like', 32330), (b'product', 28218), (b'one', 20569), (b'flavor', 19575), (b'would', 17972), (b'tri', 17753), (b'use', 15302), (b'good', 15041), (b'coffe', 14716), (b'get', 13786), (b'buy', 13752), (b'order', 12871), (b'food', 12754), (b'dont', 11877), (b'tea', 11665), (b'even', 11085), (b'box', 10844), (b'amazon', 10073), (b'make', 9840)]\n"
     ]
    }
   ],
   "source": [
    "freq_dist_positive = nltk.FreqDist(all_positive_words)\n",
    "freq_dist_negative = nltk.FreqDist(all_negative_words)\n",
    "\n",
    "print(\"Most common Positive words used: \", freq_dist_positive.most_common(20))\n",
    "print(\"Most common Negative words used: \", freq_dist_negative.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(ngram_range=(1,2))\n",
    "final_bigram_counts = count_vect.fit_transform(final['Text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 2910192)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_bigram_counts.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vect = TfidfVectorizer(ngram_range = (1, 2))\n",
    "final_tf_idf = tf_idf_vect.fit_transform(final['Text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 2910192)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_tf_idf.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2910192"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = tf_idf_vect.get_feature_names()\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ales until',\n",
       " 'ales ve',\n",
       " 'ales would',\n",
       " 'ales you',\n",
       " 'alessandra',\n",
       " 'alessandra ambrosia',\n",
       " 'alessi',\n",
       " 'alessi added',\n",
       " 'alessi also',\n",
       " 'alessi and']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[100000:100010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(final_tf_idf[3,:].toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_features(row, features, top_n = 25):\n",
    "    '''Get top n tfidf values in row and return them with their corresponding values'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_features = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_features)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "top_tfidf = top_tfidf_features(final_tf_idf[1,:].toarray()[0], features, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sendak books</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rosie movie</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>paperbacks seem</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cover version</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>these sendak</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the paperbacks</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pages open</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>really rosie</td>\n",
       "      <td>0.168074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>incorporates them</td>\n",
       "      <td>0.168074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>paperbacks</td>\n",
       "      <td>0.168074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>however miss</td>\n",
       "      <td>0.164269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hard cover</td>\n",
       "      <td>0.164269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>seem kind</td>\n",
       "      <td>0.161317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>up reading</td>\n",
       "      <td>0.156867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>that incorporates</td>\n",
       "      <td>0.155100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>the pages</td>\n",
       "      <td>0.149737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sendak</td>\n",
       "      <td>0.149737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rosie</td>\n",
       "      <td>0.146786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>of flimsy</td>\n",
       "      <td>0.146786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>two hands</td>\n",
       "      <td>0.145130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>movie that</td>\n",
       "      <td>0.144374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>reading these</td>\n",
       "      <td>0.137184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>too do</td>\n",
       "      <td>0.134491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>incorporates</td>\n",
       "      <td>0.134147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>flimsy and</td>\n",
       "      <td>0.132254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feature     tfidf\n",
       "0        sendak books  0.173437\n",
       "1         rosie movie  0.173437\n",
       "2     paperbacks seem  0.173437\n",
       "3       cover version  0.173437\n",
       "4        these sendak  0.173437\n",
       "5      the paperbacks  0.173437\n",
       "6          pages open  0.173437\n",
       "7        really rosie  0.168074\n",
       "8   incorporates them  0.168074\n",
       "9          paperbacks  0.168074\n",
       "10       however miss  0.164269\n",
       "11         hard cover  0.164269\n",
       "12          seem kind  0.161317\n",
       "13         up reading  0.156867\n",
       "14  that incorporates  0.155100\n",
       "15          the pages  0.149737\n",
       "16             sendak  0.149737\n",
       "17              rosie  0.146786\n",
       "18          of flimsy  0.146786\n",
       "19          two hands  0.145130\n",
       "20         movie that  0.144374\n",
       "21      reading these  0.137184\n",
       "22             too do  0.134491\n",
       "23       incorporates  0.134147\n",
       "24         flimsy and  0.132254"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\venne\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\venne\\anaconda3\\lib\\site-packages (from gensim)\n",
      "Requirement already satisfied: Cython==0.29.14 in c:\\users\\venne\\anaconda3\\lib\\site-packages (from gensim)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\venne\\anaconda3\\lib\\site-packages (from gensim)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\venne\\anaconda3\\lib\\site-packages (from gensim)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\venne\\anaconda3\\lib\\site-packages (from gensim)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 21.0.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-Vectors-negative300.bin.gz', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.07421875e-01, -2.01171875e-01,  1.23046875e-01,  2.11914062e-01,\n",
       "       -9.13085938e-02,  2.16796875e-01, -1.31835938e-01,  8.30078125e-02,\n",
       "        2.02148438e-01,  4.78515625e-02,  3.66210938e-02, -2.45361328e-02,\n",
       "        2.39257812e-02, -1.60156250e-01, -2.61230469e-02,  9.71679688e-02,\n",
       "       -6.34765625e-02,  1.84570312e-01,  1.70898438e-01, -1.63085938e-01,\n",
       "       -1.09375000e-01,  1.49414062e-01, -4.65393066e-04,  9.61914062e-02,\n",
       "        1.68945312e-01,  2.60925293e-03,  8.93554688e-02,  6.49414062e-02,\n",
       "        3.56445312e-02, -6.93359375e-02, -1.46484375e-01, -1.21093750e-01,\n",
       "       -2.27539062e-01,  2.45361328e-02, -1.24511719e-01, -3.18359375e-01,\n",
       "       -2.20703125e-01,  1.30859375e-01,  3.66210938e-02, -3.63769531e-02,\n",
       "       -1.13281250e-01,  1.95312500e-01,  9.76562500e-02,  1.26953125e-01,\n",
       "        6.59179688e-02,  6.93359375e-02,  1.02539062e-02,  1.75781250e-01,\n",
       "       -1.68945312e-01,  1.21307373e-03, -2.98828125e-01, -1.15234375e-01,\n",
       "        5.66406250e-02, -1.77734375e-01, -2.08984375e-01,  1.76757812e-01,\n",
       "        2.38037109e-02, -2.57812500e-01, -4.46777344e-02,  1.88476562e-01,\n",
       "        5.51757812e-02,  5.02929688e-02, -1.06933594e-01,  1.89453125e-01,\n",
       "       -1.16210938e-01,  8.49609375e-02, -1.71875000e-01,  2.45117188e-01,\n",
       "       -1.73828125e-01, -8.30078125e-03,  4.56542969e-02, -1.61132812e-02,\n",
       "        1.86523438e-01, -6.05468750e-02, -4.17480469e-02,  1.82617188e-01,\n",
       "        2.20703125e-01, -1.22558594e-01, -2.55126953e-02, -3.08593750e-01,\n",
       "        9.13085938e-02,  1.60156250e-01,  1.70898438e-01,  1.19628906e-01,\n",
       "        7.08007812e-02, -2.64892578e-02, -3.08837891e-02,  4.06250000e-01,\n",
       "       -1.01562500e-01,  5.71289062e-02, -7.26318359e-03, -9.17968750e-02,\n",
       "       -1.50390625e-01, -2.55859375e-01,  2.16796875e-01, -3.63769531e-02,\n",
       "        2.24609375e-01,  8.00781250e-02,  1.56250000e-01,  5.27343750e-02,\n",
       "        1.50390625e-01, -1.14746094e-01, -8.64257812e-02,  1.19140625e-01,\n",
       "       -7.17773438e-02,  2.73437500e-01, -1.64062500e-01,  7.29370117e-03,\n",
       "        4.21875000e-01, -1.12792969e-01, -1.35742188e-01, -1.31835938e-01,\n",
       "       -1.37695312e-01, -7.66601562e-02,  6.25000000e-02,  4.98046875e-02,\n",
       "       -1.91406250e-01, -6.03027344e-02,  2.27539062e-01,  5.88378906e-02,\n",
       "       -3.24218750e-01,  5.41992188e-02, -1.35742188e-01,  8.17871094e-03,\n",
       "       -5.24902344e-02, -1.74713135e-03, -9.81445312e-02, -2.86865234e-02,\n",
       "        3.61328125e-02,  2.15820312e-01,  5.98144531e-02, -3.08593750e-01,\n",
       "       -2.27539062e-01,  2.61718750e-01,  9.86328125e-02, -5.07812500e-02,\n",
       "        1.78222656e-02,  1.31835938e-01, -5.35156250e-01, -1.81640625e-01,\n",
       "        1.38671875e-01, -3.10546875e-01, -9.71679688e-02,  1.31835938e-01,\n",
       "       -1.16210938e-01,  7.03125000e-02,  2.85156250e-01,  3.51562500e-02,\n",
       "       -1.01562500e-01, -3.75976562e-02,  1.41601562e-01,  1.42578125e-01,\n",
       "       -5.68847656e-02,  2.65625000e-01, -2.09960938e-01,  9.64355469e-03,\n",
       "       -6.68945312e-02, -4.83398438e-02, -6.10351562e-02,  2.45117188e-01,\n",
       "       -9.66796875e-02,  1.78222656e-02, -1.27929688e-01, -4.78515625e-02,\n",
       "       -7.26318359e-03,  1.79687500e-01,  2.78320312e-02, -2.10937500e-01,\n",
       "       -1.43554688e-01, -1.27929688e-01,  1.73339844e-02, -3.60107422e-03,\n",
       "       -2.04101562e-01,  3.63159180e-03, -1.19628906e-01, -6.15234375e-02,\n",
       "        5.93261719e-02, -3.23486328e-03, -1.70898438e-01, -3.14941406e-02,\n",
       "       -8.88671875e-02, -2.89062500e-01,  3.44238281e-02, -1.87500000e-01,\n",
       "        2.94921875e-01,  1.58203125e-01, -1.19628906e-01,  7.61718750e-02,\n",
       "        6.39648438e-02, -4.68750000e-02, -6.83593750e-02,  1.21459961e-02,\n",
       "       -1.44531250e-01,  4.54101562e-02,  3.68652344e-02,  3.88671875e-01,\n",
       "        1.45507812e-01, -2.55859375e-01, -4.46777344e-02, -1.33789062e-01,\n",
       "       -1.38671875e-01,  6.59179688e-02,  1.37695312e-01,  1.14746094e-01,\n",
       "        2.03125000e-01, -4.78515625e-02,  1.80664062e-02, -8.54492188e-02,\n",
       "       -2.48046875e-01, -3.39843750e-01, -2.83203125e-02,  1.05468750e-01,\n",
       "       -2.14843750e-01, -8.74023438e-02,  7.12890625e-02,  1.87500000e-01,\n",
       "       -1.12304688e-01,  2.73437500e-01, -3.26171875e-01, -1.77734375e-01,\n",
       "       -4.24804688e-02, -2.69531250e-01,  6.64062500e-02, -6.88476562e-02,\n",
       "       -1.99218750e-01, -7.03125000e-02, -2.43164062e-01, -3.66210938e-02,\n",
       "       -7.37304688e-02, -1.77734375e-01,  9.17968750e-02, -1.25000000e-01,\n",
       "       -1.65039062e-01, -3.57421875e-01, -2.85156250e-01, -1.66992188e-01,\n",
       "        1.97265625e-01, -1.53320312e-01,  2.31933594e-02,  2.06054688e-01,\n",
       "        1.80664062e-01, -2.74658203e-02, -1.92382812e-01, -9.61914062e-02,\n",
       "       -1.06811523e-02, -4.73632812e-02,  6.54296875e-02, -1.25732422e-02,\n",
       "        1.78222656e-02, -8.00781250e-02, -2.59765625e-01,  9.37500000e-02,\n",
       "       -7.81250000e-02,  4.68750000e-02, -2.22167969e-02,  1.86767578e-02,\n",
       "        3.11279297e-02,  1.04980469e-02, -1.69921875e-01,  2.58789062e-02,\n",
       "       -3.41796875e-02, -1.44042969e-02, -5.46875000e-02, -8.78906250e-02,\n",
       "        1.96838379e-03,  2.23632812e-01, -1.36718750e-01,  1.75781250e-01,\n",
       "       -1.63085938e-01,  1.87500000e-01,  3.44238281e-02, -5.63964844e-02,\n",
       "       -2.27689743e-05,  4.27246094e-02,  5.81054688e-02, -1.07910156e-01,\n",
       "       -3.88183594e-02, -2.69531250e-01,  3.34472656e-02,  9.81445312e-02,\n",
       "        5.63964844e-02,  2.23632812e-01, -5.49316406e-02,  1.46484375e-01,\n",
       "        5.93261719e-02, -2.19726562e-01,  6.39648438e-02,  1.66015625e-02,\n",
       "        4.56542969e-02,  3.26171875e-01, -3.80859375e-01,  1.70898438e-01,\n",
       "        5.66406250e-02, -1.04492188e-01,  1.38671875e-01, -1.57226562e-01,\n",
       "        3.23486328e-03, -4.80957031e-02, -2.48046875e-01, -6.20117188e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['computer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76640123"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('woman', 'man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man', 0.7664012312889099),\n",
       " ('girl', 0.7494640946388245),\n",
       " ('teenage_girl', 0.7336829900741577),\n",
       " ('teenager', 0.631708562374115),\n",
       " ('lady', 0.6288785934448242),\n",
       " ('teenaged_girl', 0.6141784191131592),\n",
       " ('mother', 0.607630729675293),\n",
       " ('policewoman', 0.6069462299346924),\n",
       " ('boy', 0.5975908041000366),\n",
       " ('Woman', 0.5770983099937439)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('delicious', 0.8730390071868896),\n",
       " ('scrumptious', 0.8007042407989502),\n",
       " ('yummy', 0.7856923937797546),\n",
       " ('flavorful', 0.7420163154602051),\n",
       " ('delectable', 0.7385421991348267),\n",
       " ('juicy_flavorful', 0.7114803791046143),\n",
       " ('appetizing', 0.7017217874526978),\n",
       " ('crunchy_salty', 0.7012301087379456),\n",
       " ('flavourful', 0.6912213563919067),\n",
       " ('flavoursome', 0.6857703328132629)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('tasty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "i = 0\n",
    "list_of_sentences = []\n",
    "for sentence in final['Text'].values:\n",
    "    filtered_sentence = []\n",
    "    sentence = cleanhtml(sentence)\n",
    "    for w in sentence.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if(cleaned_words.isalpha()):\n",
    "                filtered_sentence.append(cleaned_words.lower())\n",
    "            else:\n",
    "                continue\n",
    "    list_of_sentences.append(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final['Text'].values[0])\n",
    "print('----------------------------')\n",
    "print(list_of_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.Word2Vec(list_of_sentences, min_count = 5, size = 50, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33783\n"
     ]
    }
   ],
   "source": [
    "words = list(w2v_model.wv.vocab)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tastey', 0.8978191018104553),\n",
       " ('yummy', 0.8643166422843933),\n",
       " ('satisfying', 0.8427529335021973),\n",
       " ('filling', 0.8251222372055054),\n",
       " ('delicious', 0.8162357211112976),\n",
       " ('flavorful', 0.7898172736167908),\n",
       " ('tasteful', 0.7695887684822083),\n",
       " ('versatile', 0.7648526430130005),\n",
       " ('addicting', 0.7619239091873169),\n",
       " ('delectable', 0.7548799514770508)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('tasty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('resemble', 0.7227350473403931),\n",
       " ('mean', 0.6622120141983032),\n",
       " ('dislike', 0.6540369987487793),\n",
       " ('prefer', 0.6520158052444458),\n",
       " ('think', 0.6218041181564331),\n",
       " ('fake', 0.6050191521644592),\n",
       " ('overpower', 0.5920742750167847),\n",
       " ('enjoy', 0.5799568891525269),\n",
       " ('miss', 0.5780380964279175),\n",
       " ('alright', 0.5727273225784302)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like\n"
     ]
    }
   ],
   "source": [
    "count_vect_feature = count_vect.get_feature_names()\n",
    "count_vect_feature.index('like')\n",
    "print(count_vect_feature[64055])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
